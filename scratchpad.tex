## Throttling design and implementation at Meta

The initial design was focused on rate-limiting where each process was set a limit on the number of requests that it can send or receive.However this was not enough, because limits such as QPS are a poor proxy to measure resource utilization. Also, some metrics can be measured more efficiently external to the application/service.

In the next iteration, the design changed to a two-tier throttling system where the initial rate-limiting with an external throttling library that controls access to core resources -- CPU, memory, network bandwidth (anything else?) -- by setting quotas for each process it monitors.

Questoins/observations:

* We need a good example to show how the initial rate-limiting design was insufficient and where it fails. Ideally, the example whould also highlight why the two-tier throttling works
[answer: rate limit doesn't work in this cases because it needs to be constantly updated based on the workload and needs to be constatly updated, also requests are heterogenous and devs cannot use one limit to satisfy all types]

* We need to draw two diagrams to show how rate-limiting initially worked, and how the two-tier rate + quota throttling works now. It's still unclear to me what resources are monitored and how (probably using probes at the OS level?). Also, it's unclear what entities are monitored and set quotas for (you mentioned each service/process runs inside it's own container and the quota is actually per-thread or did I misunderstand?)
[answer (second part): they monitor fundamental machine resources: CPU utilization, memory utilization, and network bandwidth utiliation at the OS level.]

* Ideally, we can show a breakdown of each major or critical issue based on ``missing throttling'' vs. ``mis-matched limit'' (e.g., where something like QPS was insufficient or poor proxy measurement for resource utilization).


## Company-internal bug report data set

1. Major root causes closed-source

* No throttling is one of the largest root causes: developers don't use guidances or API, so they need some external library to plug into their service/applications

* "Big request" is different: sometimes a request creates a flury of jobs; any others? (for example: query that triggers a scan to the entire DB that happens due to migration or update like adding a new column; the solution often is to split into smaller requests by the user)

* Misconfiguration are more prevalent, because QPS or rate limit based throttling is not always a good throttling strategy (see answer to the first question)

* Retry aggresively during throttling is more frequent in closed-source (what we call post-handling in open-source)

* No isolation -- multiple tenents, some tenants can be nosy and affect another tenent; usually devs set QPS on the entire service, but later change it on a "per tenant" limit; if a tenant is buggy and all of the sudden generates 10x the traffic, a whole-service QPS will prevent other tenants to operate because the ``noisy'' tenant will hog up resources. To solve it, they build "priority-based scheduling" which is a separate component in their throttling library. This doesn't appear in open-source (except maybe Cassandra)

* There are very little "mismatched metric" bugs

2. Major triggers for closed-source

Note: This is something not explained or documented in open-source bug reports 

* PRIMARY REASON: Buggy code (not related to throttling) caused the request to do more work (cascding bug); rate limit doesn't work in this cases because it needs to be constantly updated based on the workload and needs to be constatly updated, also requests are heterogenous and devs cannot use one limit to satisfy all types (first question); buggy code can cause more requests or current requests to be heavier

* Datacenter fialure which causes a rise of traffic in other datacenters

* Big requests, like migratoin or upgrades

* Under-provising (not enough machines/resourcs) which causes crashes instead of rejecting incoming traffic; these could have cascading effect. This is a very specific under-provisioning case, because if under-provisioning doesn't cause any issues but simply triggers throttling and the solution is adding more machines/resources, then that's not a throttling bug.

* Bursty user traffic is another trigger, but not too many throttling bugs are trigger this way 
