\section{Introduction}

A server application can get overloaded when it is given more requests that it can handle,
resuling in server crash or degraded performance. To prevent server overloading, existing 
server applications often involve \emph{throttling} mechanisms, which delay or block
excessive incoming requests based on certain criteria.

While the idea seems straightforward, we observe implementing throttling mechanisms
properly is non-trivial, and bugs in their implementation can lead to severe consequences.
In this paper, we study XXX throttling related bugs from Y real-world applications.

\vspace{.05in}
\noindent
\textbf{Root cause.} 

\begin{itemize}

\item No throttling. XX bugs are caused by the fact that no throttling mechanisms
were incorporated. Since the applications studied are all mature open-source projects,
they all incorporate throttling mechanisms for common requests, and thus such no-throttling
bugs mostly occur for uncommon requests, such as recovery, decomission. etc.

\item Big request. XX bugs are caused by this problem, which means a single request can
become too big in certain scenarios, causing problems like out of memory. In these cases, throttling mechanisms
have no chance to resolve the issue. To solve this type of issues, the developer needs to
break the big request into multiple smaller ones and apply throttling on smaller requests.

\item Incorrect metric. XX bugs fall into this category. It means the throttling mechanism uses
one metric as the criteria to determine whether to throttle a request, but it should another. A
typical example is using number of requests or size of requests as the metric---both can be 
insufficient in certain scenarios.

\item Misconfiguration.

\item Buggy implementation.

\end{itemize}

\vspace{.05in}
\noindent
\textbf{Symptoms.}


\subsection{Study questions}

\begin{enumerate}
    \item Is there anything surprising about how these bugs get triggered? e.g. 
    \\ (a) Are these bugs manifestations of other, possibly unrelated, issues that happened earlier in the execution and cascaded? 
    \\ (b) Do they involve multiple resources (e.g. memory pressure, high CPU utilisation, queue contention, etc.), or do they require several parts of the application to be "at capacity" at the same time? 
    \\ (c) Do they involve a small or a large number of machines/nodes? 
    \\ (d) Is the symptom easily observable (e.g. hang, crash), or it requires specialized monitoring (e.g. memory usage, flame graphs), or are there silent manifestations (e.g. metastable failures)?  
    \item What lessons can developers learn from this data set that can be applied to system design? Beyond individual anecdotes, are there any anti-patterns that developers should avoid?
    \item On a similar note, what are some system building lessons we can learn from mistakes -- i.e., the root causes?
    \item Some (most?) of these bugs have been studied by other works (e.g. performance bugs, latency/skewness, bloating, lock contention, etc.), albeit individually. Are these works lacking in some dimensions? What's distinct about this study?
    \item On a similar note, a good fraction of the bugs studied happen due to misconfigurations -- this has been thoroughly studied in the past. So, first, how can misconfiguration bug finding tools can help fix some of these bugs? And, second, what is so interesting about the rest of the bugs -- why didn't simple re-provisioning, re-configuration does not help?
\end{enumerate}
