\section{Introduction}

A server application can get overloaded when it is given more requests that it can handle,
resuling in server crash or degraded performance. To prevent server overloading, existing 
server applications often involve \emph{throttling} mechanisms, which delay or block
excessive incoming requests based on certain criteria.

While the idea seems straightforward, we observe implementing throttling mechanisms
properly is non-trivial, and bugs in their implementation can lead to severe consequences.
In this paper, we study XXX throttling related bugs from Y real-world applications.

\vspace{.05in}
\noindent
\textbf{Root cause.} 

\begin{itemize}

\item No throttling. XX bugs are caused by the fact that no throttling mechanisms
were incorporated. Since the applications studied are all mature open-source projects,
they all incorporate throttling mechanisms for common requests, and thus such no-throttling
bugs mostly occur for uncommon requests, such as recovery, decomission. etc.

\item Big request. XX bugs are caused by this problem, which means a single request can
become too big in certain scenarios, causing problems like out of memory. In these cases, throttling mechanisms
have no chance to resolve the issue. To solve this type of issues, the developer needs to
break the big request into multiple smaller ones and apply throttling on smaller requests.

\item Incorrect metric. XX bugs fall into this category. It means the throttling mechanism uses
one metric as the criteria to determine whether to throttle a request, but it should another. A
typical example is using number of requests or size of requests as the metric---both can be 
insufficient in certain scenarios.

\item Misconfiguration.

\item Buggy implementation.

\end{itemize}

\vspace{.05in}
\noindent
\textbf{Symptoms.}


\subsection{Study questions}

\begin{enumerate}
    \item \yang{What are the common root causes?}
    \item \yang{What are the common symptoms?}
    \item Is there anything surprising about how these bugs get triggered? e.g. 
    \\ (a) Are these bugs manifestations of other, possibly unrelated, issues that 
    happened earlier in the execution and cascaded? 
    \\ (b) Do they involve multiple resources (e.g. memory pressure, high CPU utilisation, queue contention, etc.), 
    or do they require several parts of the application to be "at capacity" at the same time? 
    \\ (c) Do they involve a small or a large number of machines/nodes? 
    \\ (d) Is the symptom easily observable (e.g. hang, crash), or it requires specialized monitoring 
  (e.g. memory usage, flame graphs), or are there silent manifestations (e.g. metastable failures)?  
    \\ \yang{I would say most require either a large number of concurrent requests or a few abnormally large requests (perhaps not surprising :). Many involve those ``uncommon'' requests, like recovery, connection, etc.}
    \item What lessons can developers learn from this data set that can be applied to system design? Beyond
    individual anecdotes, are there any anti-patterns that developers should avoid?
    \item On a similar note, what are some system building lessons we can learn from mistakes -- i.e., the root causes?
    \\ \yang{This one and the previous one look similar to me, so I provide some of my answers together: 1. ALL requests need to be throttled. 2. Every data structure should have a limit. 3. Indirect metric (e.g., use request rate to limit CPU/memory utilization) is risky. 4. Implementation should update metrice BEFORE any resource of a request is consumed and
AFTER all resource of the request is released.}
    \item Some (most?) of these bugs have been studied by other works (e.g. performance bugs, latency/skewness,
    bloating, lock contention, etc.), albeit individually. Are these works lacking in some dimensions? What's 
    distinct about this study? \yang{This is a bit vague. I don't know how to answer this question unless I look at a few concrete studies. But I think some of the lessons may be different.}
    \item On a similar note, a good fraction of the bugs studied happen due to misconfigurations -- this has been
    thoroughly studied in the past. So, first, how can misconfiguration bug finding tools can help fix some
    of these bugs? And, second, what is so interesting about the rest of the bugs -- why didn't simple re-provisioning,
    re-configuration does not help?
   \\ \yang{As far as I know, prior works focus on correctness misconfiguration bugs that fail some explicit checks. Misconfiguration in our work is mostly performance-related. The appropriate setting probably depends on hardware settings. In fact, I think performance-related misconfiguration is a harder but prevalent problem, but I don't know any solution except doing a comprehensive performance test.}
    \\ \yang{I don't know what you mean by ``re-configuration does not help''. I see people reconfig to fix the bug, but of course after a bug is triggered.}
    \item \yang{Is it possible to build a general-purpose throttler (e.g., either based on queues or based on resource utilization)? May be colaborate with Meta on this. My own answer: A general-purpose throttler is a good starting point and help to prevent some of the problems, but I don't think it addresses all problems as some application may want to do it in a more fine-grained manner, e.g., recovery should not consume more than 10\% of total bandwidth. General-purpose and application-specific throttlers might be complementary.}
    \item \yang{Is adaptive throttling necessary? We see a few cases arguing for this, but many cases, including the fixed version, still use fixed threshold. For example, recovery cannot consume more than 10\% of total bandwidth even when there is no other traffic, which seems not ideal.}
\end{enumerate}
