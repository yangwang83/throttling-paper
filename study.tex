\section{Study of real-world throttling bugs}

We consider the following generic model for throttling: The throttler maintains one or a few \emph{metrics}
to determine whether throttling should be applied. The metric can be resource utilization (e.g., CPU utilization, memory consumption, etc) 
of a certain component or even as simple as a counter. A task entering the system may increase the values
of these metrics and completing the task may decrease their values. Before a task enters the system,
the throttler will check metric values and maybe the potential impact of the task, and if allowing the task
to enter will increase metric values to be above certain thresholds, the throttler will either delay or reject the
task. Note that in practice, some simplified throttlers do not have all these steps. For example, some throttlers do not
have metrics and simply add a delay to every request.

In this paper, we study bugs happend in these steps.

\vspace{.1in}
\noindent
\textbf{Methodology.} We studied the bug reports of 11 applications, including Cassandra, Hadoop Common, 
HBase, HDFS, MapReduce, ZooKeeper, YARN, Ignite, Kafka, Flink, and Spark. We searched keywords including ``throttling'', ``overload'',
``rate limit'', ``out of memory'', etc, in their JIRA issues. We then read these issue reports manually to filter out those
that are not throttling bugs. We mainly filter out the following types:

\begin{itemize}

\item Generic performance issue. If an issue can cause overutilization of a certain resource, and the solution is to
optimize the application code to reduce overall resource consumption, then we consider it a generic performance issue
and do not include it in this study, because it does not affect the steps discussed above. If the solution is to delay or
reject certain tasks, then we consider it a throttling bug.

\item Permanent resource leak. We don't consider permanent resource leak, like memory leak, in this study. Throttling
may delay their effects, but does not fundamentally address this type of issues.

\item Lock contention. Lock contention can implicitly throttle incoming requests, and thus inefficient locking can cause throttling
issues. However, since lock contention is well studied, we exclude these issues from our study, except one category that
shows a similar pattern compared to other throttling bugs.

\item Load balancing. Throttling is often used together with load balancing (i.e., if a request is throttled, distribute it to
another server). However, if the solution of an issue improves load balancing but does not touch any of the steps
mentioned above, we do not include it in our study.

\end{itemize}

After fitlering, we get a total of 106 issues, including 21 from Cassandra, 9 from Hadoop Commons, 9 from HBase, 19 from HDFS, 10 from Ignite, 13 from Kafka, 
4 from MapReduce, 3 from Spark, 1 from YARN, 2 from ZooKeeper, 15 from Flink

\subsection{Symptoms}

Severity: Urgent, critial, major, normal, minor, blocker

Impact: Crash, low performance

Over or under utilization

\subsection{Root causes and solutions}

We classify the root causes into the following types. Note that some issues have more than
one root causes, so the sum of the cases below is larger than the total number of issues.

\subsubsection{Missing throttling (29 issues).} These issues are caused by the fact that throttling
is completely missing for certain types of requests. Since the applications we studied are mature
open-source products, they have applied throttling to common requests in common scenarios, and thus
such missing throttling issues often happen for uncommon requests (e.g., recovery, decommission, etc)
and/or uncommon scenarios (e.g., a large number of affected nodes/data items, very large storage space on one node, etc).

The solution to these issues is straightforward --- add throttling.

\noindent
\textbf{Lesson: Every type of tasks should be considered for throttling.}

\subsubsection{Big task (15 issues).} These issues are caused by the fact that a single (uncommon) request
can cause resource over-utilization and thus there is no appropriate way to throttle it. In 14 of them,
the corresponding system does not throttle such requests and thus the singe request causes problems. In one
of them, the corresponding system throttles such a request but then the request can never get completed.
Note that, if there is a configurable parameter to limit task size but its value is too high, we consider it a
misconfiguration issue.

There are two common patterns in this category. First, a task can read a lot of data (e.g., recover a big
table), causing OOM. Second, a task can hold a global lock for a long time, blocking all other tasks.

The solution to this category is to split the big task into multiple smaller ones. For example, a task can
read data with an iterator, reading a portion of data at a time. A long task requiring global lock can
periodically release the lock, allowing other tasks to enter.

\noindent
\textbf{Lesson: Every task should have a limit. \bogdan{I would expand here to make it more
explicit: ``limit shared queue sizes, cap message buffer lengths, manage pooled resources''.}

\subsubsection{Incorrect metric (13 issues).} These issues are caused by using incorrect metrics during
throttling. This category is often caused by using an easy-to-compute metric to approximate
the real metric. For example, to limit the memory consumption of tasks, some systems
uses the number of tasks as the metric, assuming each task has the same memory
consumption, but this is problematic when some tasks consumes significantly more resource
than others.

The solution is to switch to the right metric.

\noindent
\textbf{Lesson: Approximation is risky. Use the real metric that you intend to throttle on.}

\subsubsection{Misconfiguration (9 issues).} These issues are caused by using a suboptimal default
threshold. While setting a different value is straightforward, some issues argue a static value
won't work and thus propose adaptive method to change the threshold values or at least
provide mechanisms to allow a user to change the value at run time.

\noindent
\textbf{Lesson: These issues indicate that static thresholds are sometimes brittle, and 
software engineers should consider adaptive, runtime-tunable limits with appropriate default values.}

\subsubsection{Buggy logic (35 issues).} This category catches other bugs in the throttling logic.
There are a few common subcategories. First, some issues are caused by incorrect formula
to update metric or sleep/delay time. For example a few issues use integers to update and
store metric values, and in certain cases, some intermediate values can go below 1 and thus
become 0, causing unexpected behavior. 

Second, as mentioned at the beginning of this section,
the system should increase the metric value \emph{before} a task starts and decrease the metric 
value \emph{after} the task finishes. Some issues are caused by increasing metric value after
a task starts. If so, the throttling does not actually prevent resource over-utilization since the
resource is utilized before throttling takes effects. Similarly, if we decrease metric value before
a task fully completes, the metric value does not accurately capture resource utilization.

Other issues include a throttling configuration exists but is not applied due to missing implementation,
the limit is on uncompressed data but throttling is applied on compressed data (or vice versa), etc.

\noindent
\textbf{Lesson: 1. Use float point values to record and compute metrics.
 2. Increase metric value before a task consumes any resource and decrease
metric value after a task releases all resource.}

\subsubsection{Post handling (7 issues).} These issues are not related to the core logic of throttling, but related
to how to handle the delayed or rejected tasks. On the one hand, those tasks should not be considered
failed and should be retried. On the other hand, they should not be retried too frequently.

\noindent
\textbf{Lesson 2: Treat throttling as a recoverable condition. Use retry logic with safeguards in place, 
like limits on the number of iterations and back-off between them to avoid common retry bugs.}

\subsubsection{Possible ways ahead.} While the lessons discussed above may help, it is always helpful
if certain tools can either help us detect those bugs or prevent those bugs from manifesting. Due to the
diversity of these issues, we don't think a single method can address all of them. We discuss two potential
solutions though none is perfect.

First, instead of relying on ad-hoc throttling implementation, one can use throttlng libraries like XXX.
We observe that applications like YYY already apply this method. A mature library hopefully will eliminate
many buggy implementation issues and post handling issues. It does not directly address the missing throttling
issue, but one can apply static or dynamic analysis to identify tasks that do not apply the library. However,
it won't address the big task issue and the incorrect metric issue.

Second, one may implement a general-purpose throttler, which treats an application as a blackbox.
A module implemented either in the OS or in the routing layer can block incoming tasks when a process'
certain resource utilization grows too high. This approach will eliminate the missing throttling issues,
some of the incorrect metric issues as it directly relies on concrete resource utilizations, many of the
buggy logic issues, etc. However, it won't support fine-grained control within an application, e.g., some
tasks should have more a priority over others. It won't address the big task issue as well.


\subsection{Triggering workload}
